<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Cloud Cost Optimization Brainstorm</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background-color: #f8fafc;
      margin: 0;
      padding: 20px;
      display: flex;
      justify-content: center;
      align-items: center;
      min-height: 100vh;
    }

    .flowchart-container {
      max-width: 900px;
      width: 100%;
      background-color: #ffffff;
      border-radius: 12px;
      box-shadow: 0 6px 12px rgba(0, 0, 0, 0.1);
      padding: 30px;
    }

    .heading {
      text-align: center;
      margin-bottom: 30px;
      color: #1e293b;
      font-size: 28px;
      font-weight: 700;
    }

    .stage {
      background-color: #f1f5f9;
      border-radius: 10px;
      padding: 20px;
      margin: 15px 0;
      position: relative;
      transition: all 0.3s ease;
    }

    .stage:hover {
      background-color: #e2e8f0;
      transform: translateY(-2px);
    }

    .stage h3 {
      margin: 0 0 12px;
      color: #1e293b;
      font-size: 20px;
      font-weight: 600;
      display: flex;
      align-items: center;
    }

    .stage h3::before {
      content: attr(data-number);
      display: inline-block;
      width: 28px;
      height: 28px;
      line-height: 28px;
      text-align: center;
      margin-right: 12px;
      background-color: #2563eb;
      color: #ffffff;
      border-radius: 50%;
      font-size: 14px;
      font-weight: 600;
    }

    .stage p, .stage li {
      margin: 6px 0;
      color: #475569;
      font-size: 15px;
      line-height: 1.6;
    }

    .stage ul {
      padding-left: 20px;
      margin: 10px 0;
    }

    .questions {
      margin-top: 10px;
      padding: 10px;
      background-color: #e2e8f0;
      border-radius: 6px;
    }

    .questions li {
      font-style: italic;
      color: #1e293b;
    }

    .rationale {
      font-style: italic;
      color: #2563eb;
      margin-top: 12px;
      font-size: 14px;
      border-left: 3px solid #2563eb;
      padding-left: 10px;
    }

    .arrow {
      text-align: center;
      font-size: 30px;
      color: #2563eb;
      margin: 15px 0;
    }

    .feedback-loop {
      position: absolute;
      right: 10px;
      top: 50%;
      transform: translateY(-50%);
      font-size: 12px;
      color: #28a745;
      writing-mode: vertical-rl;
    }

    .task {
      background-color: #ffffff;
      border: 1px solid #e2e8f0;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
    }

    .task h4 {
      margin: 0 0 8px;
      color: #1e293b;
      font-size: 16px;
      font-weight: 500;
    }

    @media (max-width: 600px) {
      .flowchart-container {
        padding: 15px;
      }

      .heading {
        font-size: 24px;
      }

      .stage h3 {
        font-size: 18px;
      }

      .stage p, .stage li, .rationale {
        font-size: 13px;
      }

      .stage h3::before {
        width: 24px;
        height: 24px;
        line-height: 24px;
        font-size: 12px;
      }
    }
  </style>
</head>
<body>
  <div class="flowchart-container">
    <h1 class="heading">Cloud Cost Optimization Brainstorm</h1>
    <div class="stage objectives" data-number="1">
      <h3>Business Objectives</h3>
      <ul>
        <li>Detect early Spot Instance terminations to remove business risk.</li>
        <li>Predict cloud usage to optimize instance lifecycle.</li>
        <li><strong>Goal:</strong> Increase cost savings while maintaining service reliability.</li>
        <li><strong>Target Users:</strong>
          <ul>
            <li>Group A: IDC teams managing enterprise cloud for clients without large IT teams.</li>
            <li>Group B: FinOps teams optimizing spend across diverse workloads.</li>
          </ul>
        </li>
        <li><strong>Action Points:</strong>
          <ul>
            <li>Understand usage: Analyze historical patterns, automate scaling.</li>
            <li>Detect terminations: Proactively monitor and transition.</li>
          </ul>
        </li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Which user group benefits most from cost savings?</li>
          <li>How do we balance reliability vs. cost for IDC vs. FinOps?</li>
          <li>What’s the minimum acceptable reliability level?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Questioned user priorities and trade-offs to focus on cost savings and reliability, ensuring ML tasks align with IDC and FinOps needs.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage clarification" data-number="2">
      <h3>Clarification Questions</h3>
      <ul>
        <li><strong>Platform:</strong> Desktop platform, data in cloud.</li>
        <li><strong>Scale:</strong> Hundreds to thousands of instances over next year.</li>
        <li><strong>Performance:</strong> Latency < 1 min, scoring every 5 mins, 15-min window accuracy, aim for 95% confidence.</li>
        <li><strong>Data Sources:</strong> AWS CloudWatch, EC2 logs, billing info.</li>
        <li><strong>Data Collection:</strong> Real-time or daily batch?</li>
        <li><strong>Privacy:</strong> Ensure compliance with data regulations.</li>
        <li><strong>Bias:</strong> Mitigate bias in usage and termination predictions.</li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Is desktop sufficient, or do we need web access?</li>
          <li>What’s the max instance growth expected?</li>
          <li>Can we afford latency above 1 min for edge cases?</li>
          <li>How do we anonymize cloud log data?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Probed system constraints, scale, and ethical concerns to align design with business goals and ensure scalable, compliant performance.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage data-prep" data-number="3">
      <h3>Data Preparation</h3>
      <ul>
        <li><strong>Data Sources:</strong> Cloud log data from AWS CloudWatch, EC2 logs, billing info.</li>
        <li><strong>Transformation:</strong> Normalize (Min-Max/Z-score), impute missing data (mean/forward fill).</li>
        <li><strong>Windowing:</strong> 15-min rolling averages, lag features for time series.</li>
        <li><strong>Feature Engineering:</strong> Labels (0=alive, 1=terminated), train/val/test split, address skew (SMOTE, log-transform).</li>
        <li><strong>Storage:</strong> Cloud database or data lake.</li>
        <li><strong>Tools:</strong> AWS SageMaker, AutoML for efficiency.</li>
        <li><strong>Formats:</strong> Handle tabular data, exclude videos/images.</li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Are all data sources accessible in real-time?</li>
          <li>Is a data lake cost-effective vs. database?</li>
          <li>How much skew exists in termination data?</li>
          <li>Can AutoML handle our edge cases?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Questioned data access, storage, and preprocessing to ensure high-quality inputs, balancing speed to market with custom logic for edge cases, discussed with leadership for investment.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage model-dev" data-number="4">
      <h3>Model Development</h3>
      <ul>
        <li><strong>ML Objective:</strong> Maximize prediction of usage and termination to drive cost savings.</li>
        <li><strong>Summary:</strong> Use cloud log data for predictions.</li>
      </ul>
      <div class="task">
        <h4>Termination Detection (Binary Classification)</h4>
        <p>Monitor terminations, auto-transition in 15-min window.</p>
        <p><strong>Baseline:</strong> Logistic Regression (sigmoid loss).</p>
        <p><strong>Advanced:</strong> XGBoost, optional LSTM for temporal patterns.</p>
      </div>
      <div class="task">
        <h4>Usage Prediction (Time Series Forecasting)</h4>
        <p>Forecast usage for scaling.</p>
        <p><strong>Baseline:</strong> Linear Regression.</p>
        <p><strong>Advanced:</strong> Prophet, LSTM, XGBoost.</p>
        <p><strong>Anomaly Detection:</strong> Isolation Forest, Auto-encoder for normal behavior.</p>
      </div>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Do models need real-time input adaptability?</li>
          <li>Is high accuracy worth complex models?</li>
          <li>How do we decide on feature engineering needs?</li>
          <li>Can LSTMs handle our data distribution?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Asked about adaptability and complexity to start with simple models, advancing only if needed, ensuring balance between training time and prediction accuracy.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage model-eval" data-number="5">
      <h3>Model Evaluation</h3>
      <ul>
        <li><strong>Offline Metrics:</strong>
          <ul>
            <li>Classification: Recall, Precision, F1 Score.</li>
            <li>Forecasting: RMSE, MAPE.</li>
            <li>Anomaly: Detection Rate, False Positive Rate.</li>
          </ul>
        </li>
        <li><strong>Online Metrics:</strong>
          <ul>
            <li>Instances predicted before termination.</li>
            <li>Instances offloaded under utilization threshold.</li>
            <li>Cost savings per month per region per customer.</li>
          </ul>
        </li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Which offline metric is most critical?</li>
          <li>Do online metrics capture customer value?</li>
          <li>How many false positives are acceptable?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Probed metric relevance to ensure evaluation reflects production impact, prioritizing cost savings and reliable predictions.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage deployment" data-number="6">
      <h3>Serving & Deployment</h3>
      <ul>
        <li>Real-time scoring every 5 mins, latency < 1 min.</li>
        <li>Connect to AWS pricing API for dynamic decisions.</li>
        <li>Auto-transition, record savings impact.</li>
        <li><strong>Deployment:</strong> Cloud-based, not mobile.</li>
        <li><strong>Compression:</strong> Quantization, pruning.</li>
        <li><strong>Testing:</strong> A/B testing, shadow testing.</li>
        <li><strong>Pipeline:</strong> Real-time vs. batch trade-off.</li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>Is 5-min scoring optimal for cost vs. freshness?</li>
          <li>Can API handle peak loads?</li>
          <li>Should we prioritize real-time over batch?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Questioned scoring frequency and pipeline to optimize model freshness, ensuring minimal disruptions and measurable savings.</p>
    </div>
    <div class="arrow">↓</div>
    <div class="stage monitoring" data-number="7">
      <h3>Monitoring & Infrastructure</h3>
      <ul>
        <li>Track drift, overfitting, underfitting with MLflow, Grafana.</li>
        <li>Retrain based on accuracy and usage patterns.</li>
        <li><strong>Compression:</strong> Quantization, pruning for efficiency.</li>
        <li><strong>MLOps Principles:</strong>
          <ul>
            <li>Versioning, testing, deployment, monitoring.</li>
            <li>Loose-coupled architecture for easy updates.</li>
            <li>Containerization for reproducibility.</li>
            <li>Minimize non-determinism in training.</li>
            <li>Automation for pipelines.</li>
          </ul>
        </li>
        <li><strong>Metrics:</strong>
          <ul>
            <li>Leading: Deployment frequency, mean time to restore.</li>
            <li>Counter: Change failure rate.</li>
            <li>North-star: Lead time for changes (code commit to production).</li>
          </ul>
        </li>
        <li><strong>Trade-offs:</strong> Optimize data prep, feature engineering, GPU/TPU costs, fast failure recovery.</li>
      </ul>
      <div class="questions">
        <p><strong>Questions:</strong></p>
        <ul>
          <li>How frequently should we monitor drift?</li>
          <li>What accuracy drop triggers retraining?</li>
          <li>Are GPU costs justified for retraining?</li>
          <li>How do we measure lead time effectively?</li>
        </ul>
      </div>
      <p class="rationale">Rationale: Probed monitoring and MLOps to minimize failures, optimize resources, and achieve north-star goal of fast, reliable updates, ensuring long-term cost savings.</p>
      <div class="feedback-loop">Feedback Loop to Model Dev</div>
    </div>
  </div>
</body>
</html>
